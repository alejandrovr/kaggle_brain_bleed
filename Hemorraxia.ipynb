{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pydicom\nimport glob\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\ndef window_image(img, window_center,window_width, intercept, slope):\n    img = (img*slope +intercept)\n    img_min = window_center - window_width//2\n    img_max = window_center + window_width//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    return img \n\ndef dcm2np(dcm_file):\n    ds = pydicom.dcmread(dcm_file)\n    image = ds.pixel_array\n    window_center, window_width, intercept, slope = get_windowing(ds)\n    #the below line makes all the difference, allowing to see parenchimal details\n    image_windowed = window_image(image, window_center, window_width, intercept, slope)\n    image_windowed = image_windowed / image_windowed.max()\n    return image_windowed\n\ndef get_first_of_dicom_field_as_int(x):\n    #get x[0] as in int is x is a 'pydicom.multival.MultiValue', otherwise get int(x)\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_windowing(data):\n    dicom_fields = [data[('0028','1050')].value, #window center\n                    data[('0028','1051')].value, #window width\n                    data[('0028','1052')].value, #intercept\n                    data[('0028','1053')].value] #slope\n    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]\n\n\ndef next_batch(pf_loader_p, pf_loader_n, batch_size=100):\n    batch = []\n    for i in range(batch_size):\n        if np.random.rand()>0.55:\n            x, y = pf_loader_p.__getitem__(pos_neg=1)\n        else:\n            x, y = pf_loader_n.__getitem__(pos_neg=0)\n\n        if x.shape == (512, 512):\n            batch.append((x,y))\n\n    batch = random.sample(batch,batch_size-20)\n    batch_x = np.array([i[0] for i in batch])\n    batch_x = batch_x[:,np.newaxis,:]\n    batch_y = np.array([[0,1] if i[1]==1 else [1,0] for i in batch])\n\n    return batch_x, batch_y\n\nclass PF_Loader(Dataset):\n    def __init__(self, df):\n        \"\"\"Constructor for Loader\"\"\"\n        self.df = df\n\n    def __len__(self):\n        return len(self.df) \n\n    def __getitem__(self, pos_neg=0):\n        \"\"\"Itemgetter for Loader\"\"\"\n        data = self.df.sample(1)\n        img_name = data.iloc[0].PatientID\n        file_name = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/ID_'+img_name+'.dcm'\n        np_image = dcm2np(file_name)\n        label = pos_neg\n        return np_image, label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass BleedNet(nn.Module):\n    def __init__(self, activation='relu'):\n        super(BleedNet, self).__init__()\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'elu':\n            self.activation = nn.ELU()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=6, stride=2, padding=0),\n            self.activation,\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=4, stride=1, padding=1),\n            self.activation,\n            nn.AdaptiveMaxPool2d(60),\n        )\n        \n        self.wrap_up = nn.Sequential(\n            nn.Linear(32 * 60 * 60, 512),\n            self.activation,\n            nn.Linear(512, 2),\n        )\n\n    def forward(self, x):\n        out = self.layer1(x)\n        #print(out.size())\n        out = self.layer2(out)\n        #print(out.size())\n        out = out.reshape(out.size(0), -1)\n        #print(out.size())\n        out = self.wrap_up(out)\n        #print(out.size())\n        return out\n\ncheck_net = False\nif check_net:    \n    net = BleedNet()\n    net.to('cuda')\n    fake_input = np.random.rand(4,1,512,512)\n    fake_input = torch.from_numpy(fake_input)\n    fake_input = fake_input.float().to('cuda')\n    net.forward(fake_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport glob\nfrom copy import deepcopy\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import KFold\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport psutil\nimport pickle\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.utils import shuffle\n\ndef df2pf_loader(df):\n    df['Sub_type'] = df['ID'].str.split(\"_\", n = 3, expand = True)[2]\n    df['PatientID'] = df['ID'].str.split(\"_\", n = 3, expand = True)[1]\n    bleed_subtype_df = df.loc[df['Sub_type'] == 'any']\n\n    df_subtype_pos = bleed_subtype_df.loc[bleed_subtype_df['Label'] == 1]\n    df_subtype_neg = bleed_subtype_df.loc[bleed_subtype_df['Label'] == 0]\n\n    pf_loader_pos = PF_Loader(df_subtype_pos)\n    pf_loader_neg = PF_Loader(df_subtype_neg)\n    return pf_loader_pos, pf_loader_neg\n\n\nmake_split = True\n\nif make_split:\n    df = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv')\n    df = shuffle(df)\n\n    msk = np.random.rand(len(df)) < 0.8 #80% for training\n    train = df[msk]\n    val_test = df[~msk]\n\n    msk_val_test = np.random.rand(len(val_test)) < 0.5\n    val = val_test[msk_val_test] #10% val\n    test = val_test[~msk_val_test] #10% test\n\n    print('Train size:', len(train))\n    print('Val size:', len(val))\n    print('Test size:', len(test))\n\n    df = train\n\n#Load data\ntrain_pf_loader_pos, train_pf_loader_neg = df2pf_loader(df.sample(10000)) \n\nval_pf_loader_pos, val_pf_loader_neg = df2pf_loader(val) \ntest_pf_loader_pos, test_pf_loader_neg = df2pf_loader(test) \n\n#Learning and net parameters\nn_batches = 4000\nbatch_size = 100\nlr = 0.01\nUSE_GPU = True\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using device: {}'.format(device))\nloss_fn = nn.CrossEntropyLoss()\nbleed_net = BleedNet()\nbleed_net.to(device)\noptimizer = optim.SGD(bleed_net.parameters(), lr=lr) #momentum?\nfrom torch.optim.lr_scheduler import StepLR\nstepsize = 1000\nscheduler = StepLR(optimizer, step_size=stepsize, gamma=0.99)\n\n#Initialize logs\ntrain_loss_log = []\nval_loss_log = []\ntest_loss_log = []\n\n\n#TRAIN THE MODEL\nfor i in range(n_batches):\n    bleed_net.train()\n    x, y = next_batch(train_pf_loader_pos,train_pf_loader_neg,batch_size=batch_size)\n    x_train_tensor = torch.from_numpy(x).float().to(device)\n    y_train_tensor = torch.from_numpy(y).long().to(device)\n    y_train_tensor = y_train_tensor.argmax(dim=1)\n    yhat = bleed_net(x_train_tensor)\n    yhat_choice = yhat.argmax(dim=1)\n\n    acc = y_train_tensor == yhat_choice\n    acc = acc.sum().float() / acc.shape[0]\n    loss = loss_fn(yhat, y_train_tensor)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    scheduler.step()\n    print('Loss: {} | Acc: {} | Batch {}/{}'.format(loss.item(),acc,i,n_batches))\n    train_loss_log.append((loss.item(),acc))\n\n    if i % 100 == 0:\n        bleed_net.eval()\n        try:\n            x, y = next_batch(val_pf_loader_pos, val_pf_loader_neg, batch_size=20)\n        except:\n            continue\n        x_val_tensor = torch.from_numpy(x).float().to(device)\n        y_val_tensor = torch.from_numpy(y).long().to(device)\n        y_val_tensor = y_val_tensor.argmax(dim=1)\n        yhat = bleed_net(x_val_tensor)\n        yhat_choice = yhat.argmax(dim=1)\n\n        acc = y_val_tensor == yhat_choice\n        acc = acc.sum().float() / acc.shape[0]\n        loss = loss_fn(yhat, y_val_tensor)  \n        optimizer.zero_grad()\n        print('\\n\\n\\nVALIDATION Loss: {} | Acc: {} | Batch {}/{}\\n\\n\\n'.format(loss.item(),acc,i,n_batches))\n        val_loss_log.append((loss.item(),acc))\n\n\n#FINALLY, TEST IT\nprint('Evaluating net performance on test split...')\nbleed_net.eval()\nfor test_idx in range(10):\n    x, y = next_batch(test_pf_loader_pos, test_pf_loader_neg, batch_size=25)\n    x_test_tensor = torch.from_numpy(x).float().to(device)\n    y_test_tensor = torch.from_numpy(y).long().to(device)\n    y_test_tensor = y_test_tensor.argmax(dim=1)\n    yhat = bleed_net(x_test_tensor)\n    yhat_choice = yhat.argmax(dim=1)\n    acc = y_test_tensor == yhat_choice\n    acc = acc.sum().float() / acc.shape[0]\n    loss = loss_fn(yhat, y_test_tensor)  \n    optimizer.zero_grad()\n    print('\\n\\n\\nTEST Loss: {} | Acc: {} | Batch {}/{}\\n\\n\\n'.format(loss.item(),acc,i,n_batches))\n    test_loss_log.append((loss.item(),acc))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Saving the model...')\nimport time\ntimestr = time.strftime(\"%Y%m%d-%H%M%S\")\ntorch.save(bleed_net.state_dict(), 'models/bleednet_testacc_{}_{}.torch'.format(acc,timestr))\nprint('Model saved in:','models/bleednet_testacc_{}_{}.torch'.format(acc,timestr))\n\nplt.plot([i[0] for i in train_loss_log])\nplt.plot([i[1] for i in val_loss_log])\nplt.plot([i[1] for i in test_loss_log])\nplt.show()\nprint('Exiting...')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}