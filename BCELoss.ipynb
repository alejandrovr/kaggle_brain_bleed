{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pydicom\nimport glob\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\ndef window_image(img, window_center,window_width, intercept, slope):\n    img = (img*slope +intercept)\n    img_min = window_center - window_width//2\n    img_max = window_center + window_width//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    return img \n\ndef dcm2np(dcm_file):\n    ds = pydicom.dcmread(dcm_file)\n    image = ds.pixel_array\n    window_center, window_width, intercept, slope = get_windowing(ds)\n    #the below line makes all the difference, allowing to see parenchimal details\n    image_windowed = window_image(image, window_center, window_width, intercept, slope)\n    image_windowed = image_windowed / image_windowed.max()\n    return image_windowed\n\ndef get_first_of_dicom_field_as_int(x):\n    #get x[0] as in int is x is a 'pydicom.multival.MultiValue', otherwise get int(x)\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_windowing(data):\n    dicom_fields = [data[('0028','1050')].value, #window center\n                    data[('0028','1051')].value, #window width\n                    data[('0028','1052')].value, #intercept\n                    data[('0028','1053')].value] #slope\n    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]\n\n\ndef next_batch(typed_pf_loaders, batch_size=100):\n    all_types = ['epidural', 'intraparenchymal', 'intraventricular',\n                 'subarachnoid', 'subdural']\n    batch = []\n    for hemtype in typed_pf_loaders:\n        pf_loader_p, pf_loader_n = typed_pf_loaders[hemtype]\n        for i in range(batch_size//6):\n            final_label = np.zeros(6)\n            if i % 2 == 0:\n                x = pf_loader_p.__getitem__(pos_neg=1)\n                final_label[all_types.index(hemtype)] = 1.\n                final_label[-1] = 1. #any also true\n            else:\n                x = pf_loader_n.__getitem__(pos_neg=0)\n                \n            if x.shape == (224, 224, 3):\n                x = x.transpose(2,0,1)\n                batch.append((x,final_label))\n\n    batch_x = np.array([i[0] for i in batch])\n    #batch_x = batch_x[:,np.newaxis,:]\n    batch_y = np.array([i[1] for i in batch])\n\n    return batch_x, batch_y\n\n\nclass PF_Loader(Dataset):\n    def __init__(self, df):\n        \"\"\"Constructor for Loader\"\"\"\n        self.df = df\n\n    def __len__(self):\n        return len(self.df) \n\n    def __getitem__(self, pos_neg=0):\n        \"\"\"Itemgetter for Loader\"\"\"\n        data = self.df.sample(1)\n        img_name = data.iloc[0].PatientID\n        file_name = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/ID_'+img_name+'.dcm'\n        np_image = dcm2np(file_name)\n        from scipy import ndimage\n        zoomed_out = ndimage.zoom(np_image, 0.45)\n        zoomed_out = zoomed_out[:224,:224]\n        cm = plt.get_cmap('gist_rainbow')\n        zoomed_out = cm(zoomed_out)\n        zoomed_out = zoomed_out[:, :, :3]\n        return zoomed_out","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.cuda.is_available()\nimport os\nimport pickle\nimport random\nimport glob\nfrom copy import deepcopy\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import KFold\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport psutil\nimport pickle\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom torchvision import datasets, models, transforms\n\ndef df2pf_loader(df, subtype='any'):\n    df['Sub_type'] = df['ID'].str.split(\"_\", n = 3, expand = True)[2]\n    df['PatientID'] = df['ID'].str.split(\"_\", n = 3, expand = True)[1]\n    bleed_subtype_df = df.loc[df['Sub_type'] == subtype]\n\n    df_subtype_pos = bleed_subtype_df.loc[bleed_subtype_df['Label'] == 1]\n    df_subtype_neg = bleed_subtype_df.loc[bleed_subtype_df['Label'] == 0]\n\n    pf_loader_pos = PF_Loader(df_subtype_pos)\n    pf_loader_neg = PF_Loader(df_subtype_neg)\n    return pf_loader_pos, pf_loader_neg\n\n\ndf = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv')\ndf = df.sample(250000)\n\nmsk = np.random.rand(len(df)) < 0.8 #80% for training\ntrain = df[msk]\nval_test = df[~msk]\n\nmsk_val_test = np.random.rand(len(val_test)) < 0.5\nval = val_test[msk_val_test] #10% val\ntest = val_test[~msk_val_test] #10% test\n\nprint('Train size:', len(train))\nprint('Val size:', len(val))\nprint('Test size:', len(test))\n\ndf = train\n\nimport torch.nn.functional as F    \ndef my_loss(y_pred, y_true,device='cuda'):\n    weights = [[1.0, 1.0, 1.0, 1.0, 1.0, 2.0]] * y_pred.shape[0]\n    weights = np.array(weights)\n    weights = torch.from_numpy(weights).float()\n    weights = weights.to(device)\n    return F.binary_cross_entropy_with_logits(y_pred, y_true, weights)\n    \n#Load data\ntyped_pf_loaders = {}\nbleeding_types =  ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\nfor hem_type in bleeding_types:\n    train_pf_loader_pos, train_pf_loader_neg = df2pf_loader(df.sample(100000), subtype=hem_type) \n    typed_pf_loaders[hem_type] = [train_pf_loader_pos, train_pf_loader_neg]","execution_count":2,"outputs":[{"output_type":"stream","text":"Train size: 200067\nVal size: 24907\nTest size: 25026\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Learning and net parameters\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlr = 0.01\nn_batches = 900\nbatch_size = 32\nbleed_net = models.resnet18(pretrained=True)\nnum_ftrs = bleed_net.fc.in_features\nbleed_net.fc = nn.Linear(num_ftrs, 6)\nbleed_net = bleed_net.to(device)\noptimizer = optim.SGD(bleed_net.parameters(), lr=lr)\nloss_fn = my_loss\n\nfrom torch.optim.lr_scheduler import StepLR\nstepsize = 100 \nlr_gamma = 0.99\nscheduler = StepLR(optimizer, step_size=stepsize, gamma=lr_gamma)\n\n#Initialize logs\ntrain_loss_log = []\nval_loss_log = []\ntest_loss_log = []\n\n#TRAIN THE MODEL\nfor i in range(n_batches):\n    bleed_net.train()\n    x, y = next_batch(typed_pf_loaders,batch_size=batch_size)\n    x_train_tensor = torch.from_numpy(x).float().to(device)\n    y_train_tensor = torch.from_numpy(y).float().to(device)\n    yhat = bleed_net(x_train_tensor)\n    if i % 100 == 0:\n        yhat_any_pred = yhat[:,-1].detach().cpu().numpy().flatten().tolist()\n        gt_any = y_train_tensor[:,-1].cpu().numpy().flatten().tolist()\n        for ypred,yreal in zip(yhat_any_pred,gt_any):\n            print(ypred,yreal)\n    \n    loss = loss_fn(yhat, y_train_tensor)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    scheduler.step()\n    print('Loss: {} | Batch {}/{}'.format(loss.item(),i,n_batches))\n    train_loss_log.append(loss.item())\n    \nprint('Training finished.')\n\n\n#TODO: apply softmax before submission!","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/.cache/torch/checkpoints/resnet18-5c106cde.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 112MB/s]\n","name":"stderr"},{"output_type":"stream","text":"-0.14790363609790802 1.0\n0.2123090624809265 0.0\n-0.6982162594795227 1.0\n-0.49528801441192627 0.0\n-0.8293451070785522 1.0\n-0.8112273812294006 1.0\n-0.11899876594543457 0.0\n-0.279494047164917 1.0\n-0.07821743190288544 0.0\n-0.1453905552625656 1.0\n-0.4209209084510803 1.0\n-0.8332934975624084 0.0\n-0.02719990909099579 1.0\n-0.37929636240005493 0.0\n-0.33453643321990967 1.0\n-0.34086424112319946 1.0\n-0.7805141806602478 0.0\n0.2364392727613449 1.0\n0.23321451246738434 0.0\n-0.01272091269493103 1.0\n-0.04558075964450836 1.0\n0.0936700850725174 0.0\n-0.02679884433746338 1.0\n-0.6820119619369507 0.0\n-0.14240768551826477 1.0\nLoss: 0.7104156017303467 | Batch 0/900\nLoss: 0.6574836373329163 | Batch 1/900\nLoss: 0.6389825344085693 | Batch 2/900\nLoss: 0.6283395290374756 | Batch 3/900\nLoss: 0.5728054046630859 | Batch 4/900\nLoss: 0.5797837972640991 | Batch 5/900\nLoss: 0.5388105511665344 | Batch 6/900\nLoss: 0.5666735172271729 | Batch 7/900\nLoss: 0.5699052810668945 | Batch 8/900\nLoss: 0.5297257900238037 | Batch 9/900\nLoss: 0.5204849243164062 | Batch 10/900\nLoss: 0.5036661624908447 | Batch 11/900\nLoss: 0.4943046569824219 | Batch 12/900\nLoss: 0.5322765707969666 | Batch 13/900\nLoss: 0.5268139839172363 | Batch 14/900\nLoss: 0.507781982421875 | Batch 15/900\nLoss: 0.5454930663108826 | Batch 16/900\nLoss: 0.497732549905777 | Batch 17/900\nLoss: 0.4771477282047272 | Batch 18/900\nLoss: 0.5018730759620667 | Batch 19/900\nLoss: 0.46824148297309875 | Batch 20/900\nLoss: 0.507318913936615 | Batch 21/900\nLoss: 0.48429200053215027 | Batch 22/900\nLoss: 0.46206986904144287 | Batch 23/900\nLoss: 0.4949369430541992 | Batch 24/900\nLoss: 0.4966445565223694 | Batch 25/900\nLoss: 0.5018978118896484 | Batch 26/900\nLoss: 0.4884919226169586 | Batch 27/900\nLoss: 0.45837774872779846 | Batch 28/900\nLoss: 0.49235618114471436 | Batch 29/900\nLoss: 0.47157716751098633 | Batch 30/900\nLoss: 0.4899708032608032 | Batch 31/900\nLoss: 0.4908027648925781 | Batch 32/900\nLoss: 0.45632031559944153 | Batch 33/900\nLoss: 0.45492249727249146 | Batch 34/900\nLoss: 0.4787425994873047 | Batch 35/900\nLoss: 0.5299754738807678 | Batch 36/900\nLoss: 0.5261970162391663 | Batch 37/900\nLoss: 0.49191102385520935 | Batch 38/900\nLoss: 0.4681170880794525 | Batch 39/900\nLoss: 0.4621051251888275 | Batch 40/900\nLoss: 0.44456085562705994 | Batch 41/900\nLoss: 0.42037832736968994 | Batch 42/900\nLoss: 0.509658932685852 | Batch 43/900\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/matplotlib/colors.py:512: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n","name":"stderr"},{"output_type":"stream","text":"Loss: 0.45279067754745483 | Batch 44/900\nLoss: 0.4899994432926178 | Batch 45/900\nLoss: 0.4782494306564331 | Batch 46/900\nLoss: 0.48284414410591125 | Batch 47/900\nLoss: 0.479348361492157 | Batch 48/900\nLoss: 0.47949305176734924 | Batch 49/900\nLoss: 0.4396929442882538 | Batch 50/900\nLoss: 0.44036439061164856 | Batch 51/900\nLoss: 0.4833606481552124 | Batch 52/900\nLoss: 0.4999653100967407 | Batch 53/900\nLoss: 0.4443829357624054 | Batch 54/900\nLoss: 0.451754629611969 | Batch 55/900\nLoss: 0.47793814539909363 | Batch 56/900\nLoss: 0.449314683675766 | Batch 57/900\nLoss: 0.41079047322273254 | Batch 58/900\nLoss: 0.4408605098724365 | Batch 59/900\nLoss: 0.4192027747631073 | Batch 60/900\nLoss: 0.3983316123485565 | Batch 61/900\nLoss: 0.4531762897968292 | Batch 62/900\nLoss: 0.42835235595703125 | Batch 63/900\nLoss: 0.4392414391040802 | Batch 64/900\nLoss: 0.4472474157810211 | Batch 65/900\nLoss: 0.502885103225708 | Batch 66/900\nLoss: 0.4427412152290344 | Batch 67/900\nLoss: 0.4061276614665985 | Batch 68/900\nLoss: 0.4107915759086609 | Batch 69/900\nLoss: 0.46671345829963684 | Batch 70/900\nLoss: 0.46863311529159546 | Batch 71/900\nLoss: 0.4476557970046997 | Batch 72/900\nLoss: 0.44799479842185974 | Batch 73/900\nLoss: 0.4288516342639923 | Batch 74/900\nLoss: 0.4361312985420227 | Batch 75/900\nLoss: 0.5110809206962585 | Batch 76/900\nLoss: 0.4324852228164673 | Batch 77/900\nLoss: 0.41382333636283875 | Batch 78/900\nLoss: 0.42276644706726074 | Batch 79/900\nLoss: 0.49506568908691406 | Batch 80/900\nLoss: 0.45040640234947205 | Batch 81/900\nLoss: 0.4401412010192871 | Batch 82/900\nLoss: 0.4290783405303955 | Batch 83/900\nLoss: 0.4626610279083252 | Batch 84/900\nLoss: 0.44758501648902893 | Batch 85/900\nLoss: 0.4176536798477173 | Batch 86/900\nLoss: 0.48428258299827576 | Batch 87/900\nLoss: 0.46591421961784363 | Batch 88/900\nLoss: 0.49379920959472656 | Batch 89/900\nLoss: 0.43673813343048096 | Batch 90/900\nLoss: 0.46619996428489685 | Batch 91/900\nLoss: 0.4123486876487732 | Batch 92/900\nLoss: 0.4563863277435303 | Batch 93/900\nLoss: 0.4048450291156769 | Batch 94/900\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:605: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n  \"the returned array has changed.\", UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"Loss: 0.4003208875656128 | Batch 95/900\nLoss: 0.4750533103942871 | Batch 96/900\nLoss: 0.4789159893989563 | Batch 97/900\nLoss: 0.41413983702659607 | Batch 98/900\nLoss: 0.409820556640625 | Batch 99/900\n3.0965867042541504 1.0\n-1.512224793434143 0.0\n1.4533872604370117 1.0\n-2.0504543781280518 0.0\n1.402923583984375 1.0\n2.066190481185913 1.0\n0.5783424973487854 0.0\n1.5393232107162476 1.0\n-2.161595582962036 0.0\n0.9634659290313721 1.0\n0.9998089671134949 1.0\n-0.989628255367279 0.0\n1.3111385107040405 1.0\n-1.3725165128707886 0.0\n1.9299262762069702 1.0\n1.1097040176391602 1.0\n-1.2334685325622559 0.0\n0.8374959230422974 1.0\n0.5794848203659058 0.0\n2.3115270137786865 1.0\n2.1304619312286377 1.0\n-1.8145626783370972 0.0\n0.8620997667312622 1.0\n-1.6324210166931152 0.0\n1.1791465282440186 1.0\nLoss: 0.36210110783576965 | Batch 100/900\nLoss: 0.41695210337638855 | Batch 101/900\nLoss: 0.4100249409675598 | Batch 102/900\nLoss: 0.4132751524448395 | Batch 103/900\nLoss: 0.3498927652835846 | Batch 104/900\nLoss: 0.41941210627555847 | Batch 105/900\nLoss: 0.4287964999675751 | Batch 106/900\nLoss: 0.4081963300704956 | Batch 107/900\nLoss: 0.45804211497306824 | Batch 108/900\nLoss: 0.4526534080505371 | Batch 109/900\nLoss: 0.41577181220054626 | Batch 110/900\nLoss: 0.42289748787879944 | Batch 111/900\nLoss: 0.4147551953792572 | Batch 112/900\nLoss: 0.4493272006511688 | Batch 113/900\nLoss: 0.4342658519744873 | Batch 114/900\nLoss: 0.43053874373435974 | Batch 115/900\nLoss: 0.4573743939399719 | Batch 116/900\nLoss: 0.4007273316383362 | Batch 117/900\nLoss: 0.4713549315929413 | Batch 118/900\nLoss: 0.4184143841266632 | Batch 119/900\nLoss: 0.42350098490715027 | Batch 120/900\nLoss: 0.446824848651886 | Batch 121/900\nLoss: 0.40400272607803345 | Batch 122/900\nLoss: 0.4453374445438385 | Batch 123/900\nLoss: 0.4459655284881592 | Batch 124/900\nLoss: 0.40001556277275085 | Batch 125/900\nLoss: 0.40034037828445435 | Batch 126/900\nLoss: 0.43081575632095337 | Batch 127/900\nLoss: 0.4332038462162018 | Batch 128/900\nLoss: 0.4628601670265198 | Batch 129/900\nLoss: 0.40121352672576904 | Batch 130/900\nLoss: 0.4530172348022461 | Batch 131/900\nLoss: 0.46156778931617737 | Batch 132/900\nLoss: 0.39671790599823 | Batch 133/900\nLoss: 0.4306345582008362 | Batch 134/900\nLoss: 0.39891886711120605 | Batch 135/900\nLoss: 0.4566560387611389 | Batch 136/900\nLoss: 0.4260571002960205 | Batch 137/900\nLoss: 0.3820270299911499 | Batch 138/900\nLoss: 0.4553527534008026 | Batch 139/900\nLoss: 0.37367957830429077 | Batch 140/900\nLoss: 0.4288179576396942 | Batch 141/900\nLoss: 0.4234931170940399 | Batch 142/900\nLoss: 0.3741072714328766 | Batch 143/900\nLoss: 0.4092881977558136 | Batch 144/900\nLoss: 0.3996601402759552 | Batch 145/900\nLoss: 0.4145972430706024 | Batch 146/900\nLoss: 0.35771191120147705 | Batch 147/900\nLoss: 0.3889814019203186 | Batch 148/900\nLoss: 0.42341282963752747 | Batch 149/900\nLoss: 0.40740081667900085 | Batch 150/900\nLoss: 0.4024509787559509 | Batch 151/900\nLoss: 0.4000028967857361 | Batch 152/900\nLoss: 0.4666213095188141 | Batch 153/900\nLoss: 0.47317618131637573 | Batch 154/900\nLoss: 0.32133948802948 | Batch 155/900\nLoss: 0.357573926448822 | Batch 156/900\nLoss: 0.4695936143398285 | Batch 157/900\nLoss: 0.5198851227760315 | Batch 158/900\nLoss: 0.3947768807411194 | Batch 159/900\nLoss: 0.4531113803386688 | Batch 160/900\nLoss: 0.4505334496498108 | Batch 161/900\nLoss: 0.38880327343940735 | Batch 162/900\nLoss: 0.43629828095436096 | Batch 163/900\nLoss: 0.4783274531364441 | Batch 164/900\nLoss: 0.3855138123035431 | Batch 165/900\nLoss: 0.4209096431732178 | Batch 166/900\nLoss: 0.3988652527332306 | Batch 167/900\nLoss: 0.4226124882698059 | Batch 168/900\nLoss: 0.3802042305469513 | Batch 169/900\nLoss: 0.39430248737335205 | Batch 170/900\nLoss: 0.42943331599235535 | Batch 171/900\nLoss: 0.42483365535736084 | Batch 172/900\nLoss: 0.39402735233306885 | Batch 173/900\nLoss: 0.474317342042923 | Batch 174/900\nLoss: 0.4525141417980194 | Batch 175/900\nLoss: 0.43553924560546875 | Batch 176/900\nLoss: 0.45355093479156494 | Batch 177/900\nLoss: 0.4205707907676697 | Batch 178/900\nLoss: 0.40051960945129395 | Batch 179/900\nLoss: 0.37400391697883606 | Batch 180/900\nLoss: 0.43279126286506653 | Batch 181/900\nLoss: 0.3943067193031311 | Batch 182/900\nLoss: 0.44564881920814514 | Batch 183/900\nLoss: 0.3811456561088562 | Batch 184/900\nLoss: 0.41903895139694214 | Batch 185/900\nLoss: 0.4896096885204315 | Batch 186/900\nLoss: 0.43030819296836853 | Batch 187/900\nLoss: 0.3891863524913788 | Batch 188/900\nLoss: 0.37748435139656067 | Batch 189/900\nLoss: 0.3801669776439667 | Batch 190/900\nLoss: 0.4399826228618622 | Batch 191/900\nLoss: 0.3930038809776306 | Batch 192/900\nLoss: 0.42326977849006653 | Batch 193/900\nLoss: 0.4392266869544983 | Batch 194/900\nLoss: 0.47507891058921814 | Batch 195/900\nLoss: 0.3721103370189667 | Batch 196/900\nLoss: 0.40275415778160095 | Batch 197/900\nLoss: 0.41861969232559204 | Batch 198/900\nLoss: 0.37293845415115356 | Batch 199/900\n0.6312465667724609 1.0\n-0.3583373725414276 0.0\n0.9456571936607361 1.0\n-0.5981142520904541 0.0\n0.6312465667724609 1.0\n1.6770459413528442 1.0\n-1.31562077999115 0.0\n1.8446986675262451 1.0\n-1.7652989625930786 0.0\n-0.5707022547721863 1.0\n2.5076839923858643 1.0\n-1.3533775806427002 0.0\n1.0559719800949097 1.0\n-3.041602849960327 0.0\n2.811830759048462 1.0\n1.2908689975738525 1.0\n-0.35028600692749023 0.0\n1.3472565412521362 1.0\n0.470089852809906 0.0\n0.7228062152862549 1.0\n0.27129340171813965 1.0\n-0.18192124366760254 0.0\n1.1063932180404663 1.0\n2.202411651611328 0.0\n2.055426836013794 1.0\nLoss: 0.406138151884079 | Batch 200/900\nLoss: 0.370161235332489 | Batch 201/900\nLoss: 0.3871990144252777 | Batch 202/900\nLoss: 0.3967821002006531 | Batch 203/900\nLoss: 0.4646710455417633 | Batch 204/900\nLoss: 0.40280574560165405 | Batch 205/900\nLoss: 0.40722161531448364 | Batch 206/900\nLoss: 0.35352057218551636 | Batch 207/900\nLoss: 0.4316627085208893 | Batch 208/900\nLoss: 0.39139628410339355 | Batch 209/900\nLoss: 0.39322933554649353 | Batch 210/900\nLoss: 0.4665525555610657 | Batch 211/900\nLoss: 0.4036506116390228 | Batch 212/900\nLoss: 0.4617726802825928 | Batch 213/900\nLoss: 0.45774537324905396 | Batch 214/900\nLoss: 0.3963085412979126 | Batch 215/900\nLoss: 0.3935369849205017 | Batch 216/900\nLoss: 0.43298768997192383 | Batch 217/900\nLoss: 0.45898523926734924 | Batch 218/900\nLoss: 0.3490738868713379 | Batch 219/900\nLoss: 0.3593958616256714 | Batch 220/900\nLoss: 0.3940066695213318 | Batch 221/900\nLoss: 0.4146574139595032 | Batch 222/900\nLoss: 0.36659547686576843 | Batch 223/900\nLoss: 0.4389902949333191 | Batch 224/900\nLoss: 0.40644019842147827 | Batch 225/900\nLoss: 0.34041351079940796 | Batch 226/900\nLoss: 0.3931105434894562 | Batch 227/900\nLoss: 0.35435569286346436 | Batch 228/900\nLoss: 0.3963298797607422 | Batch 229/900\nLoss: 0.40970250964164734 | Batch 230/900\nLoss: 0.3937777578830719 | Batch 231/900\nLoss: 0.4259412884712219 | Batch 232/900\nLoss: 0.4424036741256714 | Batch 233/900\nLoss: 0.434988409280777 | Batch 234/900\nLoss: 0.4562958776950836 | Batch 235/900\nLoss: 0.4194755554199219 | Batch 236/900\nLoss: 0.3246123194694519 | Batch 237/900\nLoss: 0.39949172735214233 | Batch 238/900\nLoss: 0.3674919307231903 | Batch 239/900\nLoss: 0.3945729732513428 | Batch 240/900\nLoss: 0.4076855480670929 | Batch 241/900\nLoss: 0.42845994234085083 | Batch 242/900\nLoss: 0.3789447546005249 | Batch 243/900\nLoss: 0.4321514964103699 | Batch 244/900\nLoss: 0.36280956864356995 | Batch 245/900\nLoss: 0.4574543237686157 | Batch 246/900\nLoss: 0.36062124371528625 | Batch 247/900\nLoss: 0.4288070797920227 | Batch 248/900\nLoss: 0.49061688780784607 | Batch 249/900\nLoss: 0.466641902923584 | Batch 250/900\nLoss: 0.3718148171901703 | Batch 251/900\nLoss: 0.3936871886253357 | Batch 252/900\nLoss: 0.3557044565677643 | Batch 253/900\nLoss: 0.4028758704662323 | Batch 254/900\nLoss: 0.37026429176330566 | Batch 255/900\nLoss: 0.3826979100704193 | Batch 256/900\nLoss: 0.4503023326396942 | Batch 257/900\nLoss: 0.42572107911109924 | Batch 258/900\nLoss: 0.36107102036476135 | Batch 259/900\nLoss: 0.3581271469593048 | Batch 260/900\nLoss: 0.36738088726997375 | Batch 261/900\nLoss: 0.4153625965118408 | Batch 262/900\nLoss: 0.36857137084007263 | Batch 263/900\nLoss: 0.4253195524215698 | Batch 264/900\nLoss: 0.3876112401485443 | Batch 265/900\n","name":"stdout"},{"output_type":"stream","text":"Loss: 0.4042816758155823 | Batch 266/900\nLoss: 0.3498111665248871 | Batch 267/900\nLoss: 0.46213003993034363 | Batch 268/900\nLoss: 0.3214319944381714 | Batch 269/900\nLoss: 0.3640369772911072 | Batch 270/900\nLoss: 0.4489280879497528 | Batch 271/900\nLoss: 0.42939916253089905 | Batch 272/900\nLoss: 0.41201016306877136 | Batch 273/900\nLoss: 0.34075531363487244 | Batch 274/900\nLoss: 0.3753644824028015 | Batch 275/900\nLoss: 0.38169804215431213 | Batch 276/900\nLoss: 0.46825918555259705 | Batch 277/900\nLoss: 0.3953167498111725 | Batch 278/900\nLoss: 0.4427550435066223 | Batch 279/900\nLoss: 0.3422120213508606 | Batch 280/900\nLoss: 0.3445453345775604 | Batch 281/900\nLoss: 0.35650497674942017 | Batch 282/900\nLoss: 0.45864370465278625 | Batch 283/900\nLoss: 0.4064382016658783 | Batch 284/900\nLoss: 0.3875836431980133 | Batch 285/900\nLoss: 0.37403616309165955 | Batch 286/900\nLoss: 0.4638702869415283 | Batch 287/900\nLoss: 0.34432414174079895 | Batch 288/900\nLoss: 0.37936899065971375 | Batch 289/900\nLoss: 0.3534370958805084 | Batch 290/900\nLoss: 0.35282304883003235 | Batch 291/900\nLoss: 0.4280710816383362 | Batch 292/900\nLoss: 0.40378689765930176 | Batch 293/900\nLoss: 0.4097815752029419 | Batch 294/900\nLoss: 0.41691985726356506 | Batch 295/900\nLoss: 0.3336179554462433 | Batch 296/900\nLoss: 0.4511563777923584 | Batch 297/900\nLoss: 0.40459027886390686 | Batch 298/900\nLoss: 0.34215429425239563 | Batch 299/900\n1.7921262979507446 1.0\n-4.021973133087158 0.0\n0.19659008085727692 1.0\n-2.8761372566223145 0.0\n1.7345149517059326 1.0\n3.0007545948028564 1.0\n-2.6891720294952393 0.0\n0.1227460652589798 1.0\n0.31265050172805786 0.0\n2.728484869003296 1.0\n3.8027184009552 1.0\n-3.3012073040008545 0.0\n1.6900060176849365 1.0\n0.6396658420562744 0.0\n2.1881144046783447 1.0\n3.1894819736480713 1.0\n-0.6600995659828186 0.0\n2.6927974224090576 1.0\n-0.5913609862327576 0.0\n-0.11185862123966217 1.0\n1.150911569595337 1.0\n0.6569852232933044 0.0\n0.6523252129554749 1.0\n-2.1036794185638428 0.0\n1.3457740545272827 1.0\nLoss: 0.3580092489719391 | Batch 300/900\nLoss: 0.38554272055625916 | Batch 301/900\nLoss: 0.3679177761077881 | Batch 302/900\nLoss: 0.3715932071208954 | Batch 303/900\nLoss: 0.352345734834671 | Batch 304/900\nLoss: 0.3772304356098175 | Batch 305/900\nLoss: 0.5007320642471313 | Batch 306/900\nLoss: 0.38533106446266174 | Batch 307/900\nLoss: 0.3681919574737549 | Batch 308/900\nLoss: 0.3327617645263672 | Batch 309/900\nLoss: 0.4087246358394623 | Batch 310/900\nLoss: 0.46744003891944885 | Batch 311/900\nLoss: 0.36609798669815063 | Batch 312/900\nLoss: 0.3427909314632416 | Batch 313/900\nLoss: 0.3435394763946533 | Batch 314/900\nLoss: 0.42764848470687866 | Batch 315/900\nLoss: 0.33826178312301636 | Batch 316/900\nLoss: 0.348701536655426 | Batch 317/900\nLoss: 0.4091816842556 | Batch 318/900\nLoss: 0.3832032382488251 | Batch 319/900\nLoss: 0.3905693590641022 | Batch 320/900\nLoss: 0.40442994236946106 | Batch 321/900\nLoss: 0.4138798713684082 | Batch 322/900\nLoss: 0.36115938425064087 | Batch 323/900\nLoss: 0.3418142795562744 | Batch 324/900\nLoss: 0.35067179799079895 | Batch 325/900\nLoss: 0.585449755191803 | Batch 326/900\nLoss: 0.357679545879364 | Batch 327/900\nLoss: 0.39165040850639343 | Batch 328/900\nLoss: 0.3491289019584656 | Batch 329/900\nLoss: 0.42701733112335205 | Batch 330/900\nLoss: 0.408967524766922 | Batch 331/900\nLoss: 0.4182559847831726 | Batch 332/900\nLoss: 0.4199313819408417 | Batch 333/900\nLoss: 0.3769063353538513 | Batch 334/900\nLoss: 0.47536221146583557 | Batch 335/900\nLoss: 0.3122747838497162 | Batch 336/900\nLoss: 0.4308587312698364 | Batch 337/900\nLoss: 0.3850995898246765 | Batch 338/900\nLoss: 0.35712721943855286 | Batch 339/900\nLoss: 0.420244038105011 | Batch 340/900\nLoss: 0.3547332286834717 | Batch 341/900\nLoss: 0.4499299228191376 | Batch 342/900\nLoss: 0.359880656003952 | Batch 343/900\nLoss: 0.36634281277656555 | Batch 344/900\nLoss: 0.33249038457870483 | Batch 345/900\nLoss: 0.3639504909515381 | Batch 346/900\nLoss: 0.34810933470726013 | Batch 347/900\nLoss: 0.37837475538253784 | Batch 348/900\nLoss: 0.34155720472335815 | Batch 349/900\nLoss: 0.3767470419406891 | Batch 350/900\nLoss: 0.43319809436798096 | Batch 351/900\nLoss: 0.3536987006664276 | Batch 352/900\nLoss: 0.4070163071155548 | Batch 353/900\nLoss: 0.32634446024894714 | Batch 354/900\nLoss: 0.3707836866378784 | Batch 355/900\nLoss: 0.38464856147766113 | Batch 356/900\nLoss: 0.36451584100723267 | Batch 357/900\nLoss: 0.3681088387966156 | Batch 358/900\nLoss: 0.42203381657600403 | Batch 359/900\nLoss: 0.33320632576942444 | Batch 360/900\nLoss: 0.40813618898391724 | Batch 361/900\nLoss: 0.3718833923339844 | Batch 362/900\nLoss: 0.34846925735473633 | Batch 363/900\nLoss: 0.3925619125366211 | Batch 364/900\nLoss: 0.35288122296333313 | Batch 365/900\nLoss: 0.4310707747936249 | Batch 366/900\nLoss: 0.3188754916191101 | Batch 367/900\nLoss: 0.31587284803390503 | Batch 368/900\nLoss: 0.40207740664482117 | Batch 369/900\nLoss: 0.39304378628730774 | Batch 370/900\nLoss: 0.3383910357952118 | Batch 371/900\nLoss: 0.4573885202407837 | Batch 372/900\nLoss: 0.3650229871273041 | Batch 373/900\nLoss: 0.3416411876678467 | Batch 374/900\nLoss: 0.4019607901573181 | Batch 375/900\nLoss: 0.3781023621559143 | Batch 376/900\nLoss: 0.347324937582016 | Batch 377/900\nLoss: 0.4272463619709015 | Batch 378/900\nLoss: 0.3896222710609436 | Batch 379/900\nLoss: 0.3735525608062744 | Batch 380/900\nLoss: 0.4141443371772766 | Batch 381/900\nLoss: 0.3289029896259308 | Batch 382/900\nLoss: 0.39256083965301514 | Batch 383/900\nLoss: 0.32484471797943115 | Batch 384/900\nLoss: 0.3540330231189728 | Batch 385/900\nLoss: 0.36371418833732605 | Batch 386/900\nLoss: 0.36129021644592285 | Batch 387/900\nLoss: 0.4379478693008423 | Batch 388/900\nLoss: 0.3589065670967102 | Batch 389/900\nLoss: 0.39758315682411194 | Batch 390/900\nLoss: 0.35928040742874146 | Batch 391/900\nLoss: 0.4085705280303955 | Batch 392/900\nLoss: 0.49721020460128784 | Batch 393/900\nLoss: 0.33140233159065247 | Batch 394/900\nLoss: 0.4109416604042053 | Batch 395/900\nLoss: 0.37280774116516113 | Batch 396/900\nLoss: 0.37369126081466675 | Batch 397/900\nLoss: 0.3537715971469879 | Batch 398/900\nLoss: 0.3913910388946533 | Batch 399/900\n3.768005609512329 1.0\n-1.5545127391815186 0.0\n1.079910397529602 1.0\n0.273032009601593 0.0\n2.4522011280059814 1.0\n2.3716166019439697 1.0\n-0.734444260597229 0.0\n2.081951379776001 1.0\n0.6882016658782959 0.0\n0.655472457408905 1.0\n3.4442074298858643 1.0\n-1.2662087678909302 0.0\n1.5871995687484741 1.0\n-1.9527448415756226 0.0\n2.3863391876220703 1.0\n2.800178050994873 1.0\n-0.5936094522476196 0.0\n0.2996230125427246 1.0\n-3.2508764266967773 0.0\n2.0842554569244385 1.0\n2.30493426322937 1.0\n-5.267429351806641 0.0\n-1.286445140838623 1.0\n-0.19826851785182953 0.0\n0.259037047624588 1.0\nLoss: 0.3398756980895996 | Batch 400/900\nLoss: 0.38209444284439087 | Batch 401/900\nLoss: 0.3421643078327179 | Batch 402/900\nLoss: 0.3641451597213745 | Batch 403/900\nLoss: 0.3701324164867401 | Batch 404/900\nLoss: 0.42053866386413574 | Batch 405/900\nLoss: 0.31362825632095337 | Batch 406/900\nLoss: 0.34235459566116333 | Batch 407/900\nLoss: 0.3292458653450012 | Batch 408/900\nLoss: 0.3240581750869751 | Batch 409/900\nLoss: 0.3339189291000366 | Batch 410/900\nLoss: 0.310919851064682 | Batch 411/900\nLoss: 0.44445720314979553 | Batch 412/900\nLoss: 0.35970181226730347 | Batch 413/900\nLoss: 0.3426659107208252 | Batch 414/900\nLoss: 0.3836648762226105 | Batch 415/900\nLoss: 0.33727169036865234 | Batch 416/900\nLoss: 0.31691500544548035 | Batch 417/900\nLoss: 0.3475385010242462 | Batch 418/900\nLoss: 0.4473992586135864 | Batch 419/900\nLoss: 0.3467206358909607 | Batch 420/900\nLoss: 0.32056108117103577 | Batch 421/900\nLoss: 0.3712828457355499 | Batch 422/900\nLoss: 0.35388994216918945 | Batch 423/900\nLoss: 0.32781341671943665 | Batch 424/900\nLoss: 0.3113909363746643 | Batch 425/900\nLoss: 0.30151882767677307 | Batch 426/900\nLoss: 0.37123990058898926 | Batch 427/900\nLoss: 0.32387664914131165 | Batch 428/900\nLoss: 0.39404597878456116 | Batch 429/900\nLoss: 0.36237916350364685 | Batch 430/900\nLoss: 0.31836527585983276 | Batch 431/900\nLoss: 0.39725902676582336 | Batch 432/900\nLoss: 0.3987690210342407 | Batch 433/900\nLoss: 0.3651927709579468 | Batch 434/900\nLoss: 0.3665758967399597 | Batch 435/900\nLoss: 0.447587788105011 | Batch 436/900\n","name":"stdout"},{"output_type":"stream","text":"Loss: 0.4238719046115875 | Batch 437/900\nLoss: 0.3680320680141449 | Batch 438/900\nLoss: 0.41400885581970215 | Batch 439/900\nLoss: 0.2970896065235138 | Batch 440/900\nLoss: 0.42402005195617676 | Batch 441/900\nLoss: 0.45844635367393494 | Batch 442/900\nLoss: 0.4066469967365265 | Batch 443/900\nLoss: 0.35072195529937744 | Batch 444/900\nLoss: 0.3190588653087616 | Batch 445/900\nLoss: 0.38244473934173584 | Batch 446/900\nLoss: 0.40063726902008057 | Batch 447/900\nLoss: 0.3085257411003113 | Batch 448/900\nLoss: 0.44675537943840027 | Batch 449/900\nLoss: 0.3060677945613861 | Batch 450/900\nLoss: 0.3125413954257965 | Batch 451/900\nLoss: 0.3417947590351105 | Batch 452/900\nLoss: 0.322979211807251 | Batch 453/900\nLoss: 0.3370652198791504 | Batch 454/900\nLoss: 0.38957828283309937 | Batch 455/900\nLoss: 0.4031812250614166 | Batch 456/900\nLoss: 0.3107227683067322 | Batch 457/900\nLoss: 0.38197943568229675 | Batch 458/900\nLoss: 0.348332017660141 | Batch 459/900\nLoss: 0.4607774317264557 | Batch 460/900\nLoss: 0.37254419922828674 | Batch 461/900\nLoss: 0.35028794407844543 | Batch 462/900\nLoss: 0.36879539489746094 | Batch 463/900\nLoss: 0.3707367181777954 | Batch 464/900\nLoss: 0.3785780966281891 | Batch 465/900\nLoss: 0.2942436933517456 | Batch 466/900\nLoss: 0.2896220088005066 | Batch 467/900\nLoss: 0.41733017563819885 | Batch 468/900\nLoss: 0.3705787658691406 | Batch 469/900\nLoss: 0.3096001148223877 | Batch 470/900\nLoss: 0.30866414308547974 | Batch 471/900\nLoss: 0.317626029253006 | Batch 472/900\nLoss: 0.3121611773967743 | Batch 473/900\nLoss: 0.3622889518737793 | Batch 474/900\nLoss: 0.34025120735168457 | Batch 475/900\nLoss: 0.29835018515586853 | Batch 476/900\nLoss: 0.40889158844947815 | Batch 477/900\nLoss: 0.3804425597190857 | Batch 478/900\nLoss: 0.4181326627731323 | Batch 479/900\nLoss: 0.3626900315284729 | Batch 480/900\nLoss: 0.35142114758491516 | Batch 481/900\nLoss: 0.3408469259738922 | Batch 482/900\nLoss: 0.3753010630607605 | Batch 483/900\nLoss: 0.3538700044155121 | Batch 484/900\nLoss: 0.32285189628601074 | Batch 485/900\nLoss: 0.3460797667503357 | Batch 486/900\nLoss: 0.3654153048992157 | Batch 487/900\nLoss: 0.40242800116539 | Batch 488/900\nLoss: 0.30987870693206787 | Batch 489/900\nLoss: 0.3524869680404663 | Batch 490/900\nLoss: 0.2835055887699127 | Batch 491/900\nLoss: 0.39891883730888367 | Batch 492/900\nLoss: 0.3351435661315918 | Batch 493/900\nLoss: 0.38853034377098083 | Batch 494/900\nLoss: 0.3749777376651764 | Batch 495/900\nLoss: 0.29878106713294983 | Batch 496/900\nLoss: 0.33555877208709717 | Batch 497/900\nLoss: 0.32580146193504333 | Batch 498/900\nLoss: 0.29715725779533386 | Batch 499/900\n2.3361763954162598 1.0\n-1.3252979516983032 0.0\n-0.13262057304382324 1.0\n-4.366422653198242 0.0\n0.33041858673095703 1.0\n2.923398733139038 1.0\n0.9962190985679626 0.0\n1.9439458847045898 1.0\n-1.041893720626831 0.0\n-2.7941575050354004 1.0\n2.1955742835998535 1.0\n-4.844134330749512 0.0\n2.5928001403808594 1.0\n1.033724069595337 0.0\n3.0443553924560547 1.0\n1.5806440114974976 1.0\n-3.5565810203552246 0.0\n2.2531752586364746 1.0\n2.6104085445404053 0.0\n3.0033655166625977 1.0\n1.9174302816390991 1.0\n1.1921368837356567 0.0\n1.046705961227417 1.0\n-1.8907151222229004 0.0\n1.3632797002792358 1.0\nLoss: 0.4230889081954956 | Batch 500/900\nLoss: 0.2863384783267975 | Batch 501/900\nLoss: 0.3862283527851105 | Batch 502/900\nLoss: 0.2970395088195801 | Batch 503/900\nLoss: 0.35207653045654297 | Batch 504/900\nLoss: 0.41212794184684753 | Batch 505/900\nLoss: 0.39655160903930664 | Batch 506/900\nLoss: 0.40676799416542053 | Batch 507/900\nLoss: 0.3428772985935211 | Batch 508/900\nLoss: 0.4052182734012604 | Batch 509/900\nLoss: 0.37782371044158936 | Batch 510/900\nLoss: 0.34049010276794434 | Batch 511/900\nLoss: 0.39906206727027893 | Batch 512/900\nLoss: 0.3232020139694214 | Batch 513/900\nLoss: 0.3971122205257416 | Batch 514/900\nLoss: 0.4145904779434204 | Batch 515/900\nLoss: 0.3836734890937805 | Batch 516/900\nLoss: 0.3135432004928589 | Batch 517/900\nLoss: 0.3088444769382477 | Batch 518/900\nLoss: 0.3308145999908447 | Batch 519/900\nLoss: 0.3488365709781647 | Batch 520/900\nLoss: 0.3119175434112549 | Batch 521/900\nLoss: 0.3553881049156189 | Batch 522/900\nLoss: 0.3685493469238281 | Batch 523/900\nLoss: 0.3581406772136688 | Batch 524/900\nLoss: 0.2929908335208893 | Batch 525/900\nLoss: 0.37976834177970886 | Batch 526/900\nLoss: 0.418500691652298 | Batch 527/900\nLoss: 0.30234241485595703 | Batch 528/900\nLoss: 0.27839574217796326 | Batch 529/900\nLoss: 0.2982271909713745 | Batch 530/900\nLoss: 0.35443878173828125 | Batch 531/900\nLoss: 0.38471925258636475 | Batch 532/900\nLoss: 0.3189702033996582 | Batch 533/900\nLoss: 0.36501261591911316 | Batch 534/900\nLoss: 0.3535657525062561 | Batch 535/900\nLoss: 0.35256659984588623 | Batch 536/900\nLoss: 0.4405342638492584 | Batch 537/900\nLoss: 0.3630948066711426 | Batch 538/900\nLoss: 0.3866986632347107 | Batch 539/900\nLoss: 0.35046592354774475 | Batch 540/900\nLoss: 0.32994282245635986 | Batch 541/900\nLoss: 0.3111754357814789 | Batch 542/900\nLoss: 0.42742788791656494 | Batch 543/900\nLoss: 0.3331180810928345 | Batch 544/900\nLoss: 0.30877554416656494 | Batch 545/900\nLoss: 0.3078203797340393 | Batch 546/900\nLoss: 0.38428568840026855 | Batch 547/900\nLoss: 0.32249751687049866 | Batch 548/900\nLoss: 0.35216233134269714 | Batch 549/900\nLoss: 0.3786652088165283 | Batch 550/900\nLoss: 0.34139153361320496 | Batch 551/900\nLoss: 0.314495712518692 | Batch 552/900\nLoss: 0.34580960869789124 | Batch 553/900\nLoss: 0.3239735960960388 | Batch 554/900\nLoss: 0.33717769384384155 | Batch 555/900\nLoss: 0.31643274426460266 | Batch 556/900\nLoss: 0.3038162589073181 | Batch 557/900\nLoss: 0.29562312364578247 | Batch 558/900\nLoss: 0.5002095103263855 | Batch 559/900\nLoss: 0.3033859133720398 | Batch 560/900\nLoss: 0.2850761115550995 | Batch 561/900\nLoss: 0.31144979596138 | Batch 562/900\nLoss: 0.3534729480743408 | Batch 563/900\nLoss: 0.417008638381958 | Batch 564/900\nLoss: 0.46575650572776794 | Batch 565/900\nLoss: 0.390377402305603 | Batch 566/900\nLoss: 0.3258335590362549 | Batch 567/900\nLoss: 0.3465173840522766 | Batch 568/900\nLoss: 0.3541237413883209 | Batch 569/900\nLoss: 0.3046305775642395 | Batch 570/900\nLoss: 0.32057368755340576 | Batch 571/900\nLoss: 0.3317257761955261 | Batch 572/900\nLoss: 0.32355669140815735 | Batch 573/900\nLoss: 0.3222767114639282 | Batch 574/900\nLoss: 0.43658775091171265 | Batch 575/900\nLoss: 0.34507161378860474 | Batch 576/900\nLoss: 0.32346564531326294 | Batch 577/900\nLoss: 0.27687060832977295 | Batch 578/900\nLoss: 0.3151528537273407 | Batch 579/900\nLoss: 0.34619376063346863 | Batch 580/900\nLoss: 0.35364067554473877 | Batch 581/900\nLoss: 0.3706444501876831 | Batch 582/900\nLoss: 0.3880166709423065 | Batch 583/900\nLoss: 0.33021843433380127 | Batch 584/900\nLoss: 0.3049681782722473 | Batch 585/900\nLoss: 0.3362964391708374 | Batch 586/900\nLoss: 0.390597939491272 | Batch 587/900\nLoss: 0.3406849205493927 | Batch 588/900\nLoss: 0.4068407714366913 | Batch 589/900\nLoss: 0.33797526359558105 | Batch 590/900\nLoss: 0.2705170810222626 | Batch 591/900\nLoss: 0.30051612854003906 | Batch 592/900\nLoss: 0.27345898747444153 | Batch 593/900\nLoss: 0.3859020471572876 | Batch 594/900\nLoss: 0.37997499108314514 | Batch 595/900\nLoss: 0.2996680736541748 | Batch 596/900\nLoss: 0.3504258394241333 | Batch 597/900\nLoss: 0.3232285678386688 | Batch 598/900\nLoss: 0.3559112250804901 | Batch 599/900\n0.9776440262794495 1.0\n-2.4919703006744385 0.0\n0.5268571972846985 1.0\n-0.3616101145744324 0.0\n0.7198634743690491 1.0\n3.142791748046875 1.0\n0.8899003267288208 0.0\n2.3841187953948975 1.0\n-2.150233745574951 0.0\n3.1799399852752686 1.0\n0.9433334469795227 1.0\n-2.826080322265625 0.0\n1.1048932075500488 1.0\n-0.6948521733283997 0.0\n2.9530749320983887 1.0\n2.1041297912597656 1.0\n-1.5126558542251587 0.0\n4.831790924072266 1.0\n-1.0707279443740845 0.0\n0.9951313138008118 1.0\n1.027414083480835 1.0\n-3.065603017807007 0.0\n1.658754825592041 1.0\n-1.1734031438827515 0.0\n3.682217597961426 1.0\nLoss: 0.33158814907073975 | Batch 600/900\nLoss: 0.33893853425979614 | Batch 601/900\nLoss: 0.3280753493309021 | Batch 602/900\nLoss: 0.3063328266143799 | Batch 603/900\nLoss: 0.3257392942905426 | Batch 604/900\nLoss: 0.33423981070518494 | Batch 605/900\nLoss: 0.29770827293395996 | Batch 606/900\nLoss: 0.39018046855926514 | Batch 607/900\n","name":"stdout"},{"output_type":"stream","text":"Loss: 0.32523393630981445 | Batch 608/900\nLoss: 0.343631386756897 | Batch 609/900\nLoss: 0.33921071887016296 | Batch 610/900\nLoss: 0.2803829312324524 | Batch 611/900\nLoss: 0.30754604935646057 | Batch 612/900\nLoss: 0.2754673361778259 | Batch 613/900\nLoss: 0.36563900113105774 | Batch 614/900\nLoss: 0.34525781869888306 | Batch 615/900\nLoss: 0.32863420248031616 | Batch 616/900\nLoss: 0.35500776767730713 | Batch 617/900\nLoss: 0.29837119579315186 | Batch 618/900\nLoss: 0.3040110766887665 | Batch 619/900\nLoss: 0.3422364890575409 | Batch 620/900\nLoss: 0.34116047620773315 | Batch 621/900\nLoss: 0.3743980824947357 | Batch 622/900\nLoss: 0.3085270822048187 | Batch 623/900\nLoss: 0.29831400513648987 | Batch 624/900\nLoss: 0.4529253840446472 | Batch 625/900\nLoss: 0.3451620936393738 | Batch 626/900\nLoss: 0.302609384059906 | Batch 627/900\nLoss: 0.3011123538017273 | Batch 628/900\nLoss: 0.2871696650981903 | Batch 629/900\nLoss: 0.32763269543647766 | Batch 630/900\nLoss: 0.3739931583404541 | Batch 631/900\nLoss: 0.37602749466896057 | Batch 632/900\nLoss: 0.3222235441207886 | Batch 633/900\nLoss: 0.3928833603858948 | Batch 634/900\nLoss: 0.29928895831108093 | Batch 635/900\nLoss: 0.3728094696998596 | Batch 636/900\nLoss: 0.32952824234962463 | Batch 637/900\nLoss: 0.3297056257724762 | Batch 638/900\nLoss: 0.4052135944366455 | Batch 639/900\nLoss: 0.33682695031166077 | Batch 640/900\nLoss: 0.32577037811279297 | Batch 641/900\nLoss: 0.3706769049167633 | Batch 642/900\nLoss: 0.3016754686832428 | Batch 643/900\nLoss: 0.3422490954399109 | Batch 644/900\nLoss: 0.30160897970199585 | Batch 645/900\nLoss: 0.30047306418418884 | Batch 646/900\nLoss: 0.32034748792648315 | Batch 647/900\nLoss: 0.3062252998352051 | Batch 648/900\nLoss: 0.39527347683906555 | Batch 649/900\nLoss: 0.295475572347641 | Batch 650/900\nLoss: 0.30226826667785645 | Batch 651/900\nLoss: 0.3818795680999756 | Batch 652/900\nLoss: 0.2830345332622528 | Batch 653/900\nLoss: 0.3502112925052643 | Batch 654/900\nLoss: 0.358114629983902 | Batch 655/900\nLoss: 0.33327212929725647 | Batch 656/900\nLoss: 0.29185500741004944 | Batch 657/900\nLoss: 0.34765127301216125 | Batch 658/900\nLoss: 0.28864890336990356 | Batch 659/900\nLoss: 0.31563228368759155 | Batch 660/900\nLoss: 0.2811783254146576 | Batch 661/900\nLoss: 0.3045942187309265 | Batch 662/900\nLoss: 0.38407549262046814 | Batch 663/900\nLoss: 0.2887398302555084 | Batch 664/900\nLoss: 0.36258238554000854 | Batch 665/900\nLoss: 0.2994658052921295 | Batch 666/900\nLoss: 0.4703223705291748 | Batch 667/900\nLoss: 0.3715801537036896 | Batch 668/900\nLoss: 0.49140575528144836 | Batch 669/900\nLoss: 0.3725367486476898 | Batch 670/900\nLoss: 0.30649498105049133 | Batch 671/900\nLoss: 0.31002163887023926 | Batch 672/900\nLoss: 0.33897724747657776 | Batch 673/900\nLoss: 0.4081385135650635 | Batch 674/900\nLoss: 0.28481775522232056 | Batch 675/900\nLoss: 0.3295765221118927 | Batch 676/900\nLoss: 0.3373826742172241 | Batch 677/900\nLoss: 0.3556881248950958 | Batch 678/900\nLoss: 0.37174099683761597 | Batch 679/900\nLoss: 0.2785051465034485 | Batch 680/900\nLoss: 0.32106003165245056 | Batch 681/900\nLoss: 0.34738242626190186 | Batch 682/900\nLoss: 0.39971670508384705 | Batch 683/900\nLoss: 0.344268798828125 | Batch 684/900\nLoss: 0.31634050607681274 | Batch 685/900\nLoss: 0.33642908930778503 | Batch 686/900\nLoss: 0.3252025246620178 | Batch 687/900\nLoss: 0.28272339701652527 | Batch 688/900\nLoss: 0.32065650820732117 | Batch 689/900\nLoss: 0.33022913336753845 | Batch 690/900\nLoss: 0.34658411145210266 | Batch 691/900\nLoss: 0.27752068638801575 | Batch 692/900\nLoss: 0.31011825799942017 | Batch 693/900\nLoss: 0.41388189792633057 | Batch 694/900\nLoss: 0.35263532400131226 | Batch 695/900\nLoss: 0.3353074789047241 | Batch 696/900\nLoss: 0.25646936893463135 | Batch 697/900\nLoss: 0.41617652773857117 | Batch 698/900\nLoss: 0.40186604857444763 | Batch 699/900\n2.6968393325805664 1.0\n2.3575644493103027 0.0\n2.410287857055664 1.0\n-0.6087206602096558 0.0\n2.4064180850982666 1.0\n-0.16803334653377533 1.0\n-3.2326226234436035 0.0\n-1.435697317123413 1.0\n-2.5815770626068115 0.0\n3.6596975326538086 1.0\n2.3258538246154785 1.0\n-1.958056092262268 0.0\n2.0501327514648438 1.0\n0.06928160786628723 0.0\n4.159909248352051 1.0\n2.5765485763549805 1.0\n-3.0422658920288086 0.0\n2.5152130126953125 1.0\n-1.80071222782135 0.0\n2.5390658378601074 1.0\n0.15933044254779816 1.0\n-3.2989158630371094 0.0\n2.9739084243774414 1.0\n-3.058344841003418 0.0\n1.2452640533447266 1.0\nLoss: 0.34100064635276794 | Batch 700/900\nLoss: 0.26185187697410583 | Batch 701/900\nLoss: 0.315350204706192 | Batch 702/900\nLoss: 0.3115951716899872 | Batch 703/900\nLoss: 0.33274373412132263 | Batch 704/900\nLoss: 0.3157610595226288 | Batch 705/900\nLoss: 0.35199978947639465 | Batch 706/900\nLoss: 0.3684537410736084 | Batch 707/900\nLoss: 0.2745966613292694 | Batch 708/900\nLoss: 0.38147732615470886 | Batch 709/900\nLoss: 0.3068930506706238 | Batch 710/900\nLoss: 0.37385332584381104 | Batch 711/900\nLoss: 0.2920074164867401 | Batch 712/900\nLoss: 0.35652488470077515 | Batch 713/900\nLoss: 0.3385104537010193 | Batch 714/900\nLoss: 0.27184629440307617 | Batch 715/900\nLoss: 0.26238200068473816 | Batch 716/900\nLoss: 0.28571316599845886 | Batch 717/900\nLoss: 0.33186808228492737 | Batch 718/900\nLoss: 0.29886892437934875 | Batch 719/900\nLoss: 0.2649111747741699 | Batch 720/900\nLoss: 0.3141712248325348 | Batch 721/900\nLoss: 0.4050556421279907 | Batch 722/900\nLoss: 0.343423992395401 | Batch 723/900\nLoss: 0.3689575791358948 | Batch 724/900\nLoss: 0.35877999663352966 | Batch 725/900\nLoss: 0.3436109721660614 | Batch 726/900\nLoss: 0.29968398809432983 | Batch 727/900\nLoss: 0.32681411504745483 | Batch 728/900\nLoss: 0.3301512598991394 | Batch 729/900\nLoss: 0.38656851649284363 | Batch 730/900\nLoss: 0.3115331530570984 | Batch 731/900\nLoss: 0.3663221299648285 | Batch 732/900\nLoss: 0.3575151264667511 | Batch 733/900\nLoss: 0.3527381420135498 | Batch 734/900\nLoss: 0.3164592683315277 | Batch 735/900\nLoss: 0.32040977478027344 | Batch 736/900\nLoss: 0.3019515872001648 | Batch 737/900\nLoss: 0.31041407585144043 | Batch 738/900\nLoss: 0.2787260115146637 | Batch 739/900\nLoss: 0.32170569896698 | Batch 740/900\nLoss: 0.3431628942489624 | Batch 741/900\nLoss: 0.3329430818557739 | Batch 742/900\nLoss: 0.3013511896133423 | Batch 743/900\nLoss: 0.34966611862182617 | Batch 744/900\nLoss: 0.2943391501903534 | Batch 745/900\nLoss: 0.341871052980423 | Batch 746/900\nLoss: 0.29746317863464355 | Batch 747/900\nLoss: 0.26310133934020996 | Batch 748/900\nLoss: 0.3208763897418976 | Batch 749/900\nLoss: 0.2930068075656891 | Batch 750/900\nLoss: 0.3037872016429901 | Batch 751/900\nLoss: 0.3254375159740448 | Batch 752/900\nLoss: 0.3664012849330902 | Batch 753/900\nLoss: 0.285747766494751 | Batch 754/900\nLoss: 0.32427385449409485 | Batch 755/900\nLoss: 0.3053373396396637 | Batch 756/900\nLoss: 0.3272968530654907 | Batch 757/900\nLoss: 0.3213868737220764 | Batch 758/900\nLoss: 0.24429191648960114 | Batch 759/900\nLoss: 0.33584991097450256 | Batch 760/900\nLoss: 0.35753074288368225 | Batch 761/900\nLoss: 0.375399649143219 | Batch 762/900\nLoss: 0.28815537691116333 | Batch 763/900\nLoss: 0.3823796212673187 | Batch 764/900\nLoss: 0.36153659224510193 | Batch 765/900\nLoss: 0.3039878010749817 | Batch 766/900\nLoss: 0.31504517793655396 | Batch 767/900\nLoss: 0.32247021794319153 | Batch 768/900\nLoss: 0.3308992087841034 | Batch 769/900\nLoss: 0.36991629004478455 | Batch 770/900\nLoss: 0.3523859679698944 | Batch 771/900\nLoss: 0.32793503999710083 | Batch 772/900\nLoss: 0.27308252453804016 | Batch 773/900\nLoss: 0.3592539131641388 | Batch 774/900\nLoss: 0.3313138782978058 | Batch 775/900\nLoss: 0.3274027705192566 | Batch 776/900\nLoss: 0.24898295104503632 | Batch 777/900\nLoss: 0.3463628888130188 | Batch 778/900\nLoss: 0.26659804582595825 | Batch 779/900\nLoss: 0.3040981590747833 | Batch 780/900\nLoss: 0.39284148812294006 | Batch 781/900\nLoss: 0.3485545217990875 | Batch 782/900\nLoss: 0.3516707420349121 | Batch 783/900\nLoss: 0.38072267174720764 | Batch 784/900\nLoss: 0.3527844548225403 | Batch 785/900\nLoss: 0.32583171129226685 | Batch 786/900\nLoss: 0.3113061785697937 | Batch 787/900\nLoss: 0.3042493760585785 | Batch 788/900\nLoss: 0.32777929306030273 | Batch 789/900\nLoss: 0.32112881541252136 | Batch 790/900\nLoss: 0.36883318424224854 | Batch 791/900\n","name":"stdout"},{"output_type":"stream","text":"Loss: 0.3338601291179657 | Batch 792/900\nLoss: 0.3196426331996918 | Batch 793/900\nLoss: 0.37829792499542236 | Batch 794/900\nLoss: 0.34789758920669556 | Batch 795/900\nLoss: 0.29169684648513794 | Batch 796/900\nLoss: 0.28794628381729126 | Batch 797/900\nLoss: 0.3932211399078369 | Batch 798/900\nLoss: 0.429395467042923 | Batch 799/900\n2.204669237136841 1.0\n-0.866763174533844 0.0\n0.8708613514900208 1.0\n-1.3806105852127075 0.0\n2.778560161590576 1.0\n1.5615696907043457 1.0\n-6.175429344177246 0.0\n2.6665213108062744 1.0\n-0.09605918079614639 0.0\n3.2289063930511475 1.0\n1.9632753133773804 1.0\n-2.5731024742126465 0.0\n3.2511730194091797 1.0\n-1.2666049003601074 0.0\n2.5645945072174072 1.0\n2.5352673530578613 1.0\n-4.493488788604736 0.0\n0.8738111853599548 1.0\n-2.963772773742676 0.0\n0.5661290287971497 1.0\n3.076889991760254 1.0\n-2.851388931274414 0.0\n-1.285794973373413 1.0\n3.8090896606445312 0.0\n4.122494220733643 1.0\nLoss: 0.3242267370223999 | Batch 800/900\nLoss: 0.3479602634906769 | Batch 801/900\nLoss: 0.3945530652999878 | Batch 802/900\nLoss: 0.3550679683685303 | Batch 803/900\nLoss: 0.34122344851493835 | Batch 804/900\nLoss: 0.31389275193214417 | Batch 805/900\nLoss: 0.34425151348114014 | Batch 806/900\nLoss: 0.3216789960861206 | Batch 807/900\nLoss: 0.4191409945487976 | Batch 808/900\nLoss: 0.3380211889743805 | Batch 809/900\nLoss: 0.29669561982154846 | Batch 810/900\nLoss: 0.393842488527298 | Batch 811/900\nLoss: 0.3223033845424652 | Batch 812/900\nLoss: 0.31206318736076355 | Batch 813/900\nLoss: 0.28686362504959106 | Batch 814/900\nLoss: 0.31331008672714233 | Batch 815/900\nLoss: 0.3184295892715454 | Batch 816/900\nLoss: 0.27447187900543213 | Batch 817/900\nLoss: 0.3345159888267517 | Batch 818/900\nLoss: 0.47266262769699097 | Batch 819/900\nLoss: 0.3097241520881653 | Batch 820/900\nLoss: 0.33662939071655273 | Batch 821/900\nLoss: 0.3775109052658081 | Batch 822/900\nLoss: 0.31911927461624146 | Batch 823/900\nLoss: 0.28347715735435486 | Batch 824/900\nLoss: 0.3056081533432007 | Batch 825/900\nLoss: 0.34956443309783936 | Batch 826/900\nLoss: 0.330282062292099 | Batch 827/900\nLoss: 0.3086937963962555 | Batch 828/900\nLoss: 0.28791317343711853 | Batch 829/900\nLoss: 0.3093213438987732 | Batch 830/900\nLoss: 0.285836398601532 | Batch 831/900\nLoss: 0.2764609754085541 | Batch 832/900\nLoss: 0.31292206048965454 | Batch 833/900\nLoss: 0.2585315704345703 | Batch 834/900\nLoss: 0.34393641352653503 | Batch 835/900\nLoss: 0.3205145299434662 | Batch 836/900\nLoss: 0.3229082226753235 | Batch 837/900\nLoss: 0.26689282059669495 | Batch 838/900\nLoss: 0.29031363129615784 | Batch 839/900\nLoss: 0.3627476692199707 | Batch 840/900\nLoss: 0.3002040982246399 | Batch 841/900\nLoss: 0.2623077630996704 | Batch 842/900\nLoss: 0.2722107470035553 | Batch 843/900\nLoss: 0.3377658724784851 | Batch 844/900\nLoss: 0.28251320123672485 | Batch 845/900\nLoss: 0.2836703360080719 | Batch 846/900\nLoss: 0.38430362939834595 | Batch 847/900\nLoss: 0.3236832320690155 | Batch 848/900\nLoss: 0.31552034616470337 | Batch 849/900\nLoss: 0.37231579422950745 | Batch 850/900\nLoss: 0.2971808910369873 | Batch 851/900\nLoss: 0.39128750562667847 | Batch 852/900\nLoss: 0.2947501540184021 | Batch 853/900\nLoss: 0.3285927474498749 | Batch 854/900\nLoss: 0.26810014247894287 | Batch 855/900\nLoss: 0.2970372438430786 | Batch 856/900\nLoss: 0.39344310760498047 | Batch 857/900\nLoss: 0.2769334614276886 | Batch 858/900\nLoss: 0.2887096703052521 | Batch 859/900\nLoss: 0.29021450877189636 | Batch 860/900\nLoss: 0.3783870041370392 | Batch 861/900\nLoss: 0.3193572759628296 | Batch 862/900\nLoss: 0.2589524984359741 | Batch 863/900\nLoss: 0.34383121132850647 | Batch 864/900\nLoss: 0.3874708116054535 | Batch 865/900\nLoss: 0.3049696087837219 | Batch 866/900\nLoss: 0.30448681116104126 | Batch 867/900\nLoss: 0.3739061653614044 | Batch 868/900\nLoss: 0.33706146478652954 | Batch 869/900\nLoss: 0.32805153727531433 | Batch 870/900\nLoss: 0.28932708501815796 | Batch 871/900\nLoss: 0.33941975235939026 | Batch 872/900\nLoss: 0.3982343077659607 | Batch 873/900\nLoss: 0.326637327671051 | Batch 874/900\nLoss: 0.2485649734735489 | Batch 875/900\nLoss: 0.2545309364795685 | Batch 876/900\nLoss: 0.28229594230651855 | Batch 877/900\nLoss: 0.22399286925792694 | Batch 878/900\nLoss: 0.3650799095630646 | Batch 879/900\nLoss: 0.271504282951355 | Batch 880/900\nLoss: 0.41774454712867737 | Batch 881/900\nLoss: 0.36165836453437805 | Batch 882/900\nLoss: 0.28461652994155884 | Batch 883/900\nLoss: 0.2973453104496002 | Batch 884/900\nLoss: 0.2753823697566986 | Batch 885/900\nLoss: 0.3036877512931824 | Batch 886/900\nLoss: 0.368804395198822 | Batch 887/900\nLoss: 0.3165813386440277 | Batch 888/900\nLoss: 0.3857159912586212 | Batch 889/900\nLoss: 0.3908150792121887 | Batch 890/900\nLoss: 0.34327295422554016 | Batch 891/900\nLoss: 0.2904861867427826 | Batch 892/900\nLoss: 0.249156191945076 | Batch 893/900\nLoss: 0.3127090632915497 | Batch 894/900\nLoss: 0.2605314254760742 | Batch 895/900\nLoss: 0.3182826638221741 | Batch 896/900\nLoss: 0.28151246905326843 | Batch 897/900\nLoss: 0.30189311504364014 | Batch 898/900\nLoss: 0.3783652186393738 | Batch 899/900\nTraining finished.\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}